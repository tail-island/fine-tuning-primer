深層学習（Deep Learning）には大量の学習データが必要らしいけど、そんな大量のデータなんか持ってねーよ！

深層学習には専用の高価なGPU（Graphics Processing Unit）やTPU（Tensor Processing Unit）が必要らしいけど、そんなのを買ったりクラウドで借りたりする金なんか逆さにしても出てこねーよ！

でもStable DiffusionだChat GPTだと踊りまくった人工知能を全く知らない連中が、これからは人工知能だぞAGI（Artificial General Intelligence。汎用人工知能。人間みたいになんでもできる人工知能）だぞーとか言い出しちゃって人工知能やらされて泣きそうなあなた、ここは一発、転移学習（Transfer Learning）いかがっすか？　本稿では、データが少なかった限られたリソースでも実施可能な転移学習の基礎から応用まで一通りを、文学部日本文学科出身の文系の私が数式なしでご説明します。

使用したプログラミング言語はPythonで、作成したプログラムは[GitHub](https://github.com/tail-island/transfer-learning-primer)に置きました。お時間があるときに、実際に動かしてみてください。

# 機械学習とか深層学習とか

機械学習とか深層学習とかを知らないという場合は、申し訳ありません、[文系の文系による文系のための機械学習ガイド](https://tail-island.github.io/machine-learning-primer)を軽く読んでみてください。機械学習も深層学習も、今どきはライブラリがとてもよくできていてとても簡単なのでご安心を。転移学習をする場合は、機械学習の知識はそれほど必要ないですしね。

# 転移学習

さて、突然ですが、人生で最初のプログラミング言語を学んだ時と、そのあとに別のプログラミング言語を学んだ時のことを思い出してください。最初のプログラミング言語に比べて、二回目以降のプログラミング言語って簡単にマスターできたんじゃないでしょうか？

これを深層学習に置き換えてみると、たとえば、車と飛行機と船を画像分類（Image Classification）できるように学習した後であれば、猫と犬を画像分類する学習は簡単ってことになります。画像分類のコツを学習した後に、あとは猫と犬の違いを学習するだけなのだから、ほら、簡単そうでしょ？

こんな感じに、深層学習して作成したモデルを、新しいタスク向けに再度学習させることを転移学習と呼びます。メリットはイチから学習するより簡単なことで、深層学習においては、少ないデータでも学習可能とか、学習の時間が短いとか、計算リソースが限られていても学習が可能ということになります（大量データと長時間と高速なコンピューターがあればとても難しいタスクに挑戦できることにもなるわけですけど、そんな高度な話は本稿の対象外です）。

本稿では、この素晴らしい転移学習を、実際に手を動かしてやっていきます。

……プログラミング言語を学んだ後でも英語を学ぶのは難しいのと同じように、転移学習には、似たタスクでなければ適用できないという欠点もあるんですけどね。

# ファイン・チューニング（fine-tuning）

では、具体的にどう転移学習すればよいのか……の話の前に、すみません、なぜ転移学習が機能するのか、から始めさせてください。以下の図は、先で使用するMobileNetV2のニューラル・ネットワークを図示したものです。

![]()

この図を見て、なんだか似たような構造が何回も繰り返されているなぁと感じ取っていただけると幸いです。深層学習がなぜ「深層」学習と呼ばれるかというと、似た構造が多段に積み重なって深くなっているからなんです。

このような深い構造になっていることには意味があります。難しい判断を一発ですることは難しいから、深層学習は段階的に判断していくんですよ。たとえば画像分類の場合、下の層では、尖っている部分があるかなぁとか（これで飛行機の羽があるかと位置が分かるかもしれない）、黒い部分があるかなあとか（これでタイヤが分かるかもしれない）、のっぺりしている部分があるかなぁとか（これでボディが分かるかもしれない）みたいな単純な判断をします。上の層では、下の層で得た知識を利用して、ボディの下部に埋め込むようにタイヤがある（黒い部分がのっぺりした場所の下のほうに隣り合っている）から車じゃないかなぁとか判断していきます。

で、これって、犬と猫を分類するときにも使えるんじゃないかなぁと。尖っているから耳じゃないかなぁとか、黒いから目じゃないかなぁとか、のっぺりしているからここは毛皮ではないなぁとか。そして、黒い場所の右上や左上に尖っている場所があるので、目が吊り上がっている猫じゃないかなぁみたいな感じで。ほら、使えそうでしょ？

というわけで、学習済みのニューラル・ネットの下層から途中まではそのままで、残りを入れ替えてそこだけを学習するという手が使えそうな気がしてきて、何を隠そうこれがファイン・チューニングという転移学習の一手法なんです。

## お題は猫と犬の画像分類

というわけで、実際にファイン・チューニングしてみるわけですが、ファイン・チューニングのための学習データはどうしましょうか？　皆様が実際にファイン・チューニングするときは手元にデータがあると思うのですけど、今はデータがありませんから、データをもらってきましょう。[https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip)から、猫と犬の写真をダウンロードします（ダウンロードしたファイルは./cats-and-dogs/input/に展開しました）。このデータを使用して、猫なのか犬なのかを画像分類するようにファイン・チューニングしてみましょう。

私が使用している深層学習ライブラリはTensorFlowで、TensorFlow（というかTensorFlowが内部で使用しているライブラリのKeras）には画像をまとめて管理するデータセットを作成する`tf.keras.utils.image_dataset_from_directory()`という関数があるのでそれを使用してみます。

~~~ python
train_dataset = tf.keras.utils.image_dataset_from_directory('../input/cats_and_dogs_filtered/train', image_size=(224, 224))
validation_dataset = tf.keras.utils.image_dataset_from_directory('../input/cats_and_dogs_filtered/validation', image_size=(224, 224))
~~~

`image_size`は画像のサイズ（ファイルの画像のサイズが異なる場合はこの大きさになるように拡大縮小されます）で、その値が`(224, 224)`という何とも中途半端な値になっているのは、後述する[ImageNet](https://www.image-net.org/)の画像のサイズがこの大きさなので、合わせておいた方が精度が出そうだからです。

`train_dataset`と`validation_dataset`の2つを作成しているのは、深層学習を含む機械学習というのは問題の解き方を学ぶのではなくて入力と出力のパターンを学ぶだけで、学習データでの精度が高くても、学習データ以外のデータでは精度が低くなる（この状態を過学習と呼びます）危険性があるからです。学習に使用するデータセット（`train_dataset`）のとは別のデータセット（`valid_dataset`）で精度を測定することで、過学習していないかチェックするわけですな（本当は、ハイパー・パラメーターというどんな風に学習するかのパラメーターに過度に適合していないかを確認するためのテスト・データも必要）。

ともあれ、これでデータが用意できました。

## 元になるモデルを用意する

本稿では簡単に深層学習する手段としての転移学習について述べているので、元になるモデルは自前では作らず、どこかの誰かが大量のデータと高価なコンピューターを使用して学習したモデルをもらってきます。

TensorFlowのAPIを見ると[tf.keras.applications.*](https://www.tensorflow.org/api_docs/python/tf/keras/applications)にいくつか学習済みのモデルがありましたので、TensorFlowのチュートリアルの転移学習でも元モデルとして使用している[MobileNetV2](https://arxiv.org/abs/1801.04381)を使ってみます。ついでなので、元モデルのサマリーも出力させてその中身を見てみましょう。

~~~ python
import tensorflow as tf

base_model = tf.keras.applications.mobilenet_v2.MobileNetV2()  # 元になるモデルを取得します。
base_model.summary()  # サマリーを出力します。
~~~

上のプログラムを実行すると、元になるモデルがダウンロードされて、そのあとにこんな内容が表示されました。

~~~
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to
==================================================================================================
 input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []

 Conv1 (Conv2D)              (None, 112, 112, 32)         864       ['input_1[0][0]']

 bn_Conv1 (BatchNormalizati  (None, 112, 112, 32)         128       ['Conv1[0][0]']
 on)

（中略）

 out_relu (ReLU)             (None, 7, 7, 1280)           0         ['Conv_1_bn[0][0]']

 global_average_pooling2d (  (None, 1280)                 0         ['out_relu[0][0]']
 GlobalAveragePooling2D)

 predictions (Dense)         (None, 1000)                 1281000   ['global_average_pooling2d[0][
                                                                    0]']

==================================================================================================
Total params: 3538984 (13.50 MB)
Trainable params: 3504872 (13.37 MB)
Non-trainable params: 34112 (133.25 KB)
__________________________________________________________________________________________________
~~~

MobileNetV2について詳しく知りたい方は上で「中略」した部分を読むとか論文を読むとかしていただくことにして、転移学習で重要なのは、入力と出力の部分です。

まずは入力を見てみましょう。`input_1 (InputLayer)`というのは、TensorFlow（Keras）での入力を表しています。この行のOutput Shapeを見てみると`[(None, 224, 224, 3)]`となっています。`None`の部分を無視すると（機械学習ではデータのばらつきを抑えるために複数件のデータ単位で学習をするのですけど、その単位を後で指定できるように`None`になっています）、縦224ドットで横224ドットで赤青緑の3つの属性を持つデータ、つまり普通の画像が入力になると分かります。

出力も見てみましょう。最後の行が出力なのですけど、Output Shapeは`(None, 1000)`となっています。機械学習では「犬だと思う」といった曖昧な出力ではなくて、「犬の確率は0.3、猫の確率は0.7」みたいな出力をします。この0.3とか0.7に相当する場所が1,000個用意されているので、1,000種類の何かである確率が出力されるわけですね。APIリファレンスを読んでみたら、[ImageNet](https://www.image-net.org/)で学習したモデルがダウンロードされるみたいで、ImageNetは1,000種類の分類なので`(None, 1000)`なのも納得です。

入力は普通の画像なのでまあ良いのですけど、今から作るのは犬と猫の2種類を分類するモデルですから、出力の部分は変更しなければなりません。

## 出力層を切り離して、新しい層を追加する

TensorFlow（Keras）では、Layersプロパティを通じてモデルの層にアクセスできます。`base_model.layers[0]`とすれば最初の`input_1 (InputLayer)`を、`base_model.layers[-1]`とすれば最後の`prediction (Dense)`を取得できるわけですな。今回は最後の層を削除するわけですけど、そのためには、最後の層の入力（先ほどのサマリーのConnected toを見ると`global_average_pooling2d[0][0]`になっているので、最後から2番目の層）の`output`を入力にする新しい層を作成するだ
け（最後の層に対しては何もせずに放置する）でオッケーです。

~~~ python
model_output = tf.keras.layers.Dense(2)(base_model.layers[-2].output)  # 全結合層を追加します。
~~~

`Dense()`というのは、入力の数が何であれ（`(None, 1000)`でも`(None, 2000)`でも）、引数で指定した次元（`Dense(2)`なら`(None, 2)`）を出力する層です。あと、TensorFlow（Keras）の層は、`__call__()`メソッドを実装しているので、`Dense`クラスのインスタンスは関数と同じに使えます。だから、`Dense(2)`で返ってくるインスタンスに対して`(base_model.layers[-2].output)`で関数呼び出しができるというわけ（関数側プログラミングで言うところの関数を返す関数みたいな感じです）。

以上で、猫だと思う大きさを表現する数値と、犬だと思う大きさを表現する数値を出力できるようになりました（猫なら正の数、犬なら負の数で表現するなら`Dense(1)`でも大丈夫です。後述する損失関数を変更しなければならないけど）。

で、TensorFlow（Keras）では学習を層を束ねた`Model`クラスで実施するので、この新しい層を使用する`Model`を作成します。

~~~ python
model = tf.keras.Model(base_model.input, model_output)  # TensorFlow（Keras）のモデルを作成します。
model.summary()  # サマリーを表示します。
~~~

表示されるサマリーはこんな感じ。

~~~
__________________________________________________________________________________________________
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to
==================================================================================================
 input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []

 Conv1 (Conv2D)              (None, 112, 112, 32)         864       ['input_1[0][0]']

 bn_Conv1 (BatchNormalizati  (None, 112, 112, 32)         128       ['Conv1[0][0]']
 on)

（中略）

 out_relu (ReLU)             (None, 7, 7, 1280)           0         ['Conv_1_bn[0][0]']

 global_average_pooling2d (  (None, 1280)                 0         ['out_relu[0][0]']
 GlobalAveragePooling2D)

 dense (Dense)               (None, 2)                    2562      ['global_average_pooling2d[0][
                                                                    0]']

==================================================================================================
Total params: 2260546 (8.62 MB)
Trainable params: 2226434 (8.49 MB)
Non-trainable params: 34112 (133.25 KB)
__________________________________________________________________________________________________
~~~

最後の層のOutput Shapreが`(None, 2)`となっているので、これで成功です。出力層を変更できました。

## 学習「しない」層を設定する

このまま深層学習してもよいのですけど、それだとせっかく誰かが大量データと長い待ち時間と高速なコンピューターで深層学習していい感じになっている元モデルの層をもう一度学習することになってしまうので本稿が目標としている簡単に届きません。なので、学習「しない」層を設定しましょう。

ファイン・チューニングで実施するタスクの難しさや、ファイン・チューニングのタスクと元モデルのタスクの差の大きさを元にいろいろ考えて学習しない層を決定する……のが正しいのでしょうけど、簡単なのは、できるだけ多くの層を学習しないことから初めて、精度が出なかった場合に少しずつ学習しない範囲を狭めていくという方法です。

というわけで、新たに追加した何も学習していない層「以外」はすべて学習「しない」ことにしましょう。

~~~ python
for layer in model.layers[:-1]:  # 最初から最後の層の一つ前の層までを……
    layer.trainable = False  # trainable属性（学習対象にするか否か）をFalseにします。

model.summary(show_trainable=True)  # 学習可能かも含めてサマリーを表示します。
~~~

